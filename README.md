
# Quantum Annealing Benchmarking and Training for Chatbot Model

## Overview

This repository contains Python scripts for benchmarking and training a simple chatbot model using a simulated quantum annealing approach. The goal of this project is to use quantum-inspired optimization to effectively tune the hyperparameters of the chatbot model and evaluate its performance across different computing devices (CPU and GPU).

### Files in This Repository
- `benchmark.py`: Main entry point for benchmarking the training of the chatbot model. It stages and benchmarks the quantum annealing process to measure the training effectiveness on different devices.
- `main.py`: Script for training the chatbot model and performing inference with some sample inputs. It initializes the training parameters, preprocesses data, and saves the trained model.
- `train.py`: Contains the core implementation of the quantum annealing algorithm used for training the chatbot model, using both multiprocessing for CPU and sequential processing for GPU to optimize the training process.

## Quantum Annealing Algorithm

### Introduction
Quantum annealing is an optimization technique inspired by quantum mechanics. It involves finding the global minimum of a cost function by using a process similar to simulated annealing but utilizing quantum tunneling. In this project, we employ a simulated quantum annealing process to optimize the hyperparameters of the chatbot model, aiming to minimize the loss during training.

### Algorithm Details
- **Objective Function**: The objective is to minimize the loss of the chatbot model during training. The loss is computed using a cross-entropy function.
- **Optimization Process**:
  - The quantum-inspired optimization starts with an initial random solution and attempts to improve it iteratively.
  - A candidate solution is generated by adding a random perturbation scaled by a step size. The generated candidate is clipped within predefined bounds.
  - The acceptance of a worse solution is determined by a Metropolis criterion, which allows the algorithm to escape local minima.
  - Step size is adaptively tuned based on the current state of the optimization to balance exploration and exploitation.
  - Early termination is implemented to halt optimization when the solution stabilizes or a convergence criterion is met.
- **Simulated Annealing**: The annealing process reduces the temperature over time, decreasing the probability of accepting worse solutions as the algorithm progresses.

### Usage in Training
In the training process (`train.py`), the quantum annealing algorithm is used to optimize the following hyperparameters:
- **Input Dimension**: Dimension of the input feature vector.
- **Hidden Dimension**: Number of hidden units in the model's hidden layer.
- **Learning Rate**: Step size used by the optimizer (Adam).

The `train_model` function in `train.py` leverages multiprocessing for CPU execution, distributing the evaluation of candidate solutions across multiple processes to speed up the optimization. For GPU, the process is executed sequentially due to limitations in concurrent CUDA executions.

## How the Code Works

### `benchmark.py`
The `benchmark.py` script is used to benchmark the training of the chatbot model on different devices (CPU, GPU). The key steps include:
1. **Load Training Parameters**: Loads hyperparameters from a `training_params.json` file.
2. **Preprocess Data**: Calls the `preprocess_data` function to convert training conversations into tensors suitable for model training.
3. **Run Benchmark**: Uses the `train_model` function to train the chatbot model and measure the time taken for both CPU and GPU. This helps determine the performance gain offered by GPU over CPU.

### `main.py`
This script is used for both training and testing the chatbot model. The workflow includes:
1. **Load Data and Parameters**: Training data and parameters are loaded from JSON files.
2. **Preprocess Data**: Converts conversation data into tensors, and the vocabulary is saved for future use.
3. **Train the Model**: Calls the `train_model` function to train the chatbot model with the preprocessed data and optimized hyperparameters.
4. **Inference**: After training, a few example inputs are tested with the trained model to generate responses.

### `train.py`
The `train.py` script contains the implementation of the training process using simulated quantum annealing. The key functions are:
- **`create_model`**: Creates an instance of the `SimpleChatbotModel` with specified input, hidden, and output dimensions.
- **`train_model`**: Implements the quantum-inspired optimization process, using multiprocessing to evaluate candidate hyperparameters and select the best model configuration.
- **`evaluate_candidate`**: Evaluates a candidate model configuration by training the model and calculating the average loss over a set number of epochs.

## Running the Code

### Prerequisites
- Python 3.7+
- PyTorch
- NumPy
- tqdm

### Running Benchmarks
To benchmark the training on CPU or GPU, run:
```sh
python benchmark.py
```
Follow the prompts to choose the device for benchmarking.

### Training and Testing the Model
To train the chatbot model and perform inference, run:
```sh
python main.py
```
This will train the model and test it with a set of predefined inputs.

## Quantum Annealing Considerations
- **Multiprocessing**: The multiprocessing approach for CPU allows the evaluation of multiple candidate solutions in parallel, reducing the overall training time.
- **Early Termination**: The early termination mechanism ensures that the optimization does not run for an unnecessarily long duration once convergence is detected, saving computational resources.
- **Dynamic Step Size**: The adaptation of the step size helps maintain an appropriate balance between exploration of new areas of the solution space and exploitation of known good solutions.

## Conclusion
The simulated quantum annealing approach used in this project provides an effective way to optimize the chatbot model's hyperparameters, leveraging both CPU and GPU resources to improve the training process. The benchmarking script allows users to compare the performance of training on different devices, showcasing the efficiency gains provided by GPU acceleration.

## Author
- Virgil Vaduva

## License
This project is licensed under the MIT License.
